#' Assess prediction accuracy
#' 
#' @param object A \code{PUGS} object generated by \code{\link{predict.ECC}}.
#' @param y A matrix of the same form as the one used with
#' \code{\link{ecc}}.
#' @return A variety of multi-label classification accuracy measurements.
#' @examples \dontrun{
#'   ecc(x.train, y.train) %>% predict(x.test) %>% validate(y.test)
#' }

#' @export
validate <- function(object, y) {
  UseMethod("validate")
}

#' @export
validate.PUGS <- function(object, y)
{
      if ( !is.matrix(y) ) y <- as.matrix(y)
      if ( dim(object$preds)[1] != nrow(y) | dim(object$preds)[2] != ncol(y) )
        stop("y.pred and y must have the same number of observations (instances) and classes (labels)")
      y.hat <- summary(object, type="prob")
      # Logarithmic loss provides a steep penalty for predictions that are both confident and wrong
      # y.hat is the probability that y=1
      y.hat <- 1e-7*(y.hat==0) + y.hat
      y.hat <- y.hat - 1e-7*(y.hat==1)
      log.loss <- -1*mean(as.numeric((y*log(y.hat))+((1-y)*log(1-y.hat))))
      y.hat <- summary(object, type="class")
      # average per-text exact classification
      exact.match.ratio <- mean(apply(y.hat == y, 1, all))
      # average per-text classification with partial matches
      labelling.Fscore <- mean(apply(((y.hat==1) & (y==1)), 1, sum)/apply(((y.hat==1) | (y==1)), 1, sum))
      # average per-label classification with partial matches
      retrieval.Fscore <- mean(apply(((y.hat==1) & (y==1)), 2, sum)/apply(((y.hat==1) | (y==1)), 2, sum))
      # average per-example per-class total error
      hamming.loss <- mean(as.numeric(y.hat != y))
      return(c("Logarithmic Loss" = log.loss,
               "Exact Match Ratio" = exact.match.ratio,
               "Labelling Fscore" = labelling.Fscore,
               "Retrieval Fscore" = retrieval.Fscore,
               "Hamming Loss" = hamming.loss))
}
