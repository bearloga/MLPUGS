#' @title Assess prediction accuracy
#' @description Computes a variety of accuracy metrics for multi-label
#'   predictions. 
#' @param object A \code{PUGS} object generated by \code{\link{predict.ECC}}.
#' @param y A matrix of the same form as the one used with
#' \code{\link{ecc}}.
#' @return A variety of multi-label classification accuracy measurements.
#' @examples \dontrun{
#' x <- movies_train[, -(1:3)]
#' y <- movies_train[, 1:3]
#'
#' model_c50 <- ecc(x, y, .f = C50::C5.0)
#' predictions_c50 <- predict(model_c50, movies_test[, -(1:3)],
#'                            .f = C50::predict.C5.0, type = "prob")
#' validate(predictions_c50, movies_test[, 1:3])
#'   
#' model_rf <- ecc(x, y, .f = randomForest::randomForest)
#' predictions_rf <- predict(model_rf, movies_test[, -(1:3)],
#'                           n.iters = 1000, burn.in = 100, thin = 10,
#'                           .f = function(rF, newdata){
#'                             randomForest:::predict.randomForest(rF, newdata, type = "prob")
#'                           })
#' validate(predictions_rf, movies_test[, 1:3])
#' }
#' @export
validate <- function(object, y) {
  UseMethod("validate")
}

#' @export
validate.PUGS <- function(object, y)
{
      if ( !is.matrix(y) ) {
        y <- as.matrix(y)
      }
      if ( dim(object$preds)[1] != nrow(y) | dim(object$preds)[2] != ncol(y) ) {
        stop("prediction set and test set must have the same number of observations (instances) and classes (labels)")
      }
      y_hat <- summary(object, type = "prob") # y_hat is the probability that y = 1
      y_hat <- y_hat + 1e-7 * (y_hat == 0) - 1e-7 * (y_hat == 1) # so we don't run into problems with log
      log_loss <- -mean(as.numeric((y * log(y_hat)) + ((1-y) * log(1 - y_hat))))
      y_hat <- summary(object, type = "class")
      safe_mean <- function(x) {
        if (any(is.nan(x))) {
          message("NaNs detected when computing F-scores. This is caused by observations (instances) which do not have any labels. These obs. have been removed from F-score calculation, so we recommend placing heavier emphasis on other accuracy metrics.")
          return(mean(x[!is.nan(x)]))
        }
        return(mean(x))
      }
      return(data.frame("Logarithmic Loss" = log_loss,
                        # ^ logarithmic loss provides a steep penalty for predictions
                        #   that are both confident and wrong
                        "Exact Match Ratio" = mean(apply(y_hat == y, 1, all)),
                        # ^ average per-instance exact classification
                        "Labelling F-score" = safe_mean(apply(((y_hat == 1) & (y == 1)), 1, sum)/apply(((y_hat == 1) | (y == 1)), 1, sum)),
                        # ^ average per-instance classification with partial matches
                        "Retrieval F-score" = safe_mean(apply(((y_hat==1) & (y==1)), 2, sum)/apply(((y_hat==1) | (y==1)), 2, sum)),
                        # ^ average per-label classification with partial matches
                        "Hamming Loss" = mean(as.numeric(y_hat != y)),
                        # ^ average per-example per-class total error
                        stringsAsFactors = FALSE))
}
